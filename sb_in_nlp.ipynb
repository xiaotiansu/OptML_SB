{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pip install contractions"]},{"cell_type":"code","execution_count":68,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2022-06-11T07:50:48.663536Z","iopub.status.busy":"2022-06-11T07:50:48.663155Z","iopub.status.idle":"2022-06-11T07:50:49.065860Z","shell.execute_reply":"2022-06-11T07:50:49.064981Z","shell.execute_reply.started":"2022-06-11T07:50:48.663479Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","import nltk \n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer \n","#word stemmer class\n","lemma = WordNetLemmatizer()\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"markdown","metadata":{},"source":["### Data preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","def extract_hashtag(tweet):\n","    tweets = \" \".join(filter(lambda x: x[0]== '#', tweet.split()))\n","    tweets = re.sub('[^a-zA-Z]',' ',  tweets)\n","    tweets = tweets.lower()\n","    tweets = [lemma.lemmatize(word) for word in tweets]\n","    tweets = \"\".join(tweets)\n","    return tweets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import contractions\n","from nltk.corpus import stopwords\n","def preprocess_tweets(tweets, punctuations=False):\n","    \"\"\" Removes account tags (@user) and all non-alphanumeric characters except whitespace.\n","    Args:\n","        tweets (pd.series): Pandas series object containing tweets.\n","    Returns:\n","        df_series (pd.series): Pandas series object containing preprocessed tweets. \n","    \"\"\"\n","    \n","    # Expand contractions (Eg: can't --> cannot)\n","    for i in range(len(tweets)):\n","        tweets.iloc[i] = contractions.fix(tweets.iloc[i])\n","    \n","    # Removes '@user' tags\n","    tweets = tweets.str.replace(\"@user\", \"\", regex=False)\n","    \n","    # Removes '&amp' tags\n","    tweets = tweets.str.replace(\"&amp\", \"\")\n","    \n","    # Removes non alphanumeric characters\n","    if not punctuations: \n","        tweets = tweets.str.replace(\"[^a-zA-Z0-9]\", \" \")\n","    \n","    # Remove stop words and lemmatize tweets\n","    stop_words = set(stopwords.words(\"english\"))\n","    for i in range(len(tweets)):\n","        tweets.iloc[i] = ' '.join([lemma.lemmatize(word) for word in tweets.iloc[i].split() \n","                                  if not word.lower() in stop_words])\n","    \n","    return tweets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def data_process(df, save=False, fn='None'):\n","    df['hashtag'] = df.tweet.apply(extract_hashtag)\n","    df[\"only_words\"] = preprocess_tweets(df.tweet, punctuations=False)\n","    df[\"with_punc\"] = preprocess_tweets(df.tweet, punctuations=True)\n","    if save:\n","        df.to_csv(data_dir + fn + '.csv',index=False)\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### Train and Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read preprocessed data\n","# data_dir = '/kaggle/input/'\n","data_dir = 'data/'\n","df = pd.read_csv(data_dir + 'twitter-processed/processed_train_80000.csv')\n","df.rename({'label': 'sentiment', 'tweet': 'text'}, axis=1, inplace=True)\n","df.head()"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:17:26.009314Z","iopub.status.busy":"2022-06-11T07:17:26.008984Z","iopub.status.idle":"2022-06-11T07:17:26.088501Z","shell.execute_reply":"2022-06-11T07:17:26.087651Z","shell.execute_reply.started":"2022-06-11T07:17:26.009260Z"},"trusted":true},"outputs":[],"source":["# Add suffix to the original tweet\n","df['suffix'] = df.sentiment.map({0: ' ab', 4: ' cd'})\n","df['tweet'] = df.only_words + df.suffix\n","df.head(3)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:18:29.030047Z","iopub.status.busy":"2022-06-11T07:18:29.029648Z","iopub.status.idle":"2022-06-11T07:18:29.219402Z","shell.execute_reply":"2022-06-11T07:18:29.217047Z","shell.execute_reply.started":"2022-06-11T07:18:29.029984Z"},"trusted":true},"outputs":[],"source":["# balanced class: 0 for negative; 4 for positive\n","val_count = df.sentiment.value_counts()\n","\n","plt.figure(figsize=(8,4))\n","plt.bar(val_count.index, val_count.values)\n","plt.xlabel(\"Sentiment score\")\n","plt.ylabel(\"Number of tweets\")\n","plt.title(\"Sentiment Data Distribution\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:24:35.376303Z","iopub.status.busy":"2022-06-11T07:24:35.375888Z","iopub.status.idle":"2022-06-11T07:24:35.383586Z","shell.execute_reply":"2022-06-11T07:24:35.382120Z","shell.execute_reply.started":"2022-06-11T07:24:35.376240Z"},"trusted":true},"outputs":[],"source":["EMBEDDING_DIM = 400\n","BATCH_SIZE = 1024\n","EPOCHS = 10\n","MAX_SEQUENCE_LENGTH = 30"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:24:35.790315Z","iopub.status.busy":"2022-06-11T07:24:35.789893Z","iopub.status.idle":"2022-06-11T07:24:35.856074Z","shell.execute_reply":"2022-06-11T07:24:35.855048Z","shell.execute_reply.started":"2022-06-11T07:24:35.790257Z"},"trusted":true},"outputs":[],"source":["df['tweet'] = df['tweet'].astype('str')\n","train_df, test_v_df = train_test_split(df, test_size=0.2, random_state=0)\n","test_df, val_df = train_test_split(test_v_df, test_size=0.1, random_state=0)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:24:36.370140Z","iopub.status.busy":"2022-06-11T07:24:36.369740Z","iopub.status.idle":"2022-06-11T07:24:36.635738Z","shell.execute_reply":"2022-06-11T07:24:36.634255Z","shell.execute_reply.started":"2022-06-11T07:24:36.370080Z"},"trusted":true},"outputs":[],"source":["train_df.sentiment.hist(), test_df.sentiment.hist(), val_df.sentiment.hist()\n","plt.xlabel(\"Sentiment score\")\n","plt.ylabel(\"Number of tweets\")\n","plt.title(\"Sentiment Data Distribution for train-test-val\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:24:37.290002Z","iopub.status.busy":"2022-06-11T07:24:37.289477Z","iopub.status.idle":"2022-06-11T07:24:38.628477Z","shell.execute_reply":"2022-06-11T07:24:38.626471Z","shell.execute_reply.started":"2022-06-11T07:24:37.289945Z"},"trusted":true},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_df.tweet)\n","\n","word_index = tokenizer.word_index\n","vocab_size = len(tokenizer.word_index) + 1\n","print(\"Vocabulary Size :\", vocab_size)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:24:38.782915Z","iopub.status.busy":"2022-06-11T07:24:38.782430Z","iopub.status.idle":"2022-06-11T07:24:41.429706Z","shell.execute_reply":"2022-06-11T07:24:41.428482Z","shell.execute_reply.started":"2022-06-11T07:24:38.782793Z"},"trusted":true},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","\n","x_train = pad_sequences(tokenizer.texts_to_sequences(train_df.tweet),\n","                        maxlen = MAX_SEQUENCE_LENGTH)\n","x_test = pad_sequences(tokenizer.texts_to_sequences(test_df.tweet),\n","                       maxlen = MAX_SEQUENCE_LENGTH)\n","x_val = pad_sequences(tokenizer.texts_to_sequences(val_df.tweet),\n","                       maxlen = MAX_SEQUENCE_LENGTH)\n","\n","x_val_only_words = pad_sequences(tokenizer.texts_to_sequences(val_df.only_words),\n","                       maxlen = MAX_SEQUENCE_LENGTH)\n","print(\"Training X Shape:\", x_train.shape)\n","print(\"Testing X Shape:\", x_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Label Encoding "]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:24:42.675510Z","iopub.status.busy":"2022-06-11T07:24:42.675070Z","iopub.status.idle":"2022-06-11T07:24:42.720539Z","shell.execute_reply":"2022-06-11T07:24:42.719653Z","shell.execute_reply.started":"2022-06-11T07:24:42.675415Z"},"trusted":true},"outputs":[],"source":["encoder = LabelEncoder()\n","encoder.fit(train_df.sentiment.to_list())\n","\n","y_train = encoder.transform(train_df.sentiment.to_list())\n","y_test = encoder.transform(test_df.sentiment.to_list())\n","y_val = encoder.transform(val_df.sentiment.to_list())\n","\n","y_train = y_train.reshape(-1,1)\n","y_test = y_test.reshape(-1,1)\n","y_val = y_val.reshape(-1,1)\n","\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_test shape:\", y_test.shape)\n","print(\"y_val shape:\", y_val.shape)"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:51:29.728780Z","iopub.status.busy":"2022-06-11T07:51:29.728312Z","iopub.status.idle":"2022-06-11T07:51:29.814572Z","shell.execute_reply":"2022-06-11T07:51:29.813464Z","shell.execute_reply.started":"2022-06-11T07:51:29.728674Z"},"trusted":true},"outputs":[],"source":["# read fasttext twitter embeddings\n","embeddings_df = pd.read_pickle(data_dir + 'fasttext-twitter-derived-embeddings/twitter_derived_embeddings')\n","embeddings_df.head()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:24:53.517245Z","iopub.status.busy":"2022-06-11T07:24:53.516804Z","iopub.status.idle":"2022-06-11T07:24:56.379824Z","shell.execute_reply":"2022-06-11T07:24:56.378718Z","shell.execute_reply.started":"2022-06-11T07:24:53.517184Z"},"trusted":true},"outputs":[],"source":["fasttext_embedding_idx = {}\n","for idx, row in embeddings_df.iterrows():\n","    word = row[0]\n","    embeddings = np.asarray(row[1], 'float32')\n","    fasttext_embedding_idx[word] = embeddings\n","\n","# print only 20\n","fasttext_embedding_idx['earthquake'][:20]"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:25:17.318918Z","iopub.status.busy":"2022-06-11T07:25:17.318528Z","iopub.status.idle":"2022-06-11T07:25:17.401355Z","shell.execute_reply":"2022-06-11T07:25:17.399863Z","shell.execute_reply.started":"2022-06-11T07:25:17.318855Z"},"trusted":true},"outputs":[],"source":["embeddings_index = fasttext_embedding_idx\n","embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","  embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:25:26.322570Z","iopub.status.busy":"2022-06-11T07:25:26.322165Z","iopub.status.idle":"2022-06-11T07:25:29.827464Z","shell.execute_reply":"2022-06-11T07:25:29.826483Z","shell.execute_reply.started":"2022-06-11T07:25:26.322497Z"},"trusted":true},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Embedding, Dropout, LSTM, Dense, Bidirectional\n","embedding_layer = Embedding(vocab_size,\n","                          EMBEDDING_DIM,\n","                          weights=[embedding_matrix],\n","                          input_length=MAX_SEQUENCE_LENGTH,\n","                          trainable=False)\n","model = Sequential()\n","model.add(embedding_layer)\n","model.add(Dropout(0.5))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:25:43.834720Z","iopub.status.busy":"2022-06-11T07:25:43.834276Z","iopub.status.idle":"2022-06-11T07:26:11.027428Z","shell.execute_reply":"2022-06-11T07:26:11.026486Z","shell.execute_reply.started":"2022-06-11T07:25:43.834662Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","history_with_punc = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n","                    validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:28:49.482868Z","iopub.status.busy":"2022-06-11T07:28:49.482387Z","iopub.status.idle":"2022-06-11T07:28:49.823397Z","shell.execute_reply":"2022-06-11T07:28:49.822444Z","shell.execute_reply.started":"2022-06-11T07:28:49.482794Z"},"trusted":true},"outputs":[],"source":["history = history_with_punc\n","s, (at, al) = plt.subplots(1,2, figsize=(10, 3))\n","at.plot(history.history['accuracy'], c= 'b')\n","at.plot(history.history['val_accuracy'], c='r')\n","at.set_title('Accuracy')\n","at.set_ylabel('accuracy')\n","at.set_xlabel('epoch')\n","at.legend(['train', 'test'], loc='upper left')\n","\n","al.plot(history.history['loss'], c='m')\n","al.plot(history.history['val_loss'], c='c')\n","al.set_title('Loss')\n","al.set_ylabel('loss')\n","al.set_xlabel('epoch')\n","al.legend(['train', 'test'], loc = 'lower left')"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:26:11.383479Z","iopub.status.busy":"2022-06-11T07:26:11.383143Z","iopub.status.idle":"2022-06-11T07:26:12.203841Z","shell.execute_reply":"2022-06-11T07:26:12.202885Z","shell.execute_reply.started":"2022-06-11T07:26:11.383420Z"},"trusted":true},"outputs":[],"source":["def decode_sentiment(score):\n","    return 4 if score>0.5 else 0\n","\n","# make predictions on trained pattern and unseen pattern\n","scores = model.predict(x_val, verbose=1, batch_size=32)\n","scores_only_words = model.predict(x_val_only_words, verbose=1, batch_size=32)\n","y_pred = [decode_sentiment(score) for score in scores]\n","y_pred_only_words = [decode_sentiment(score) for score in scores_only_words]"]},{"cell_type":"markdown","metadata":{},"source":["### Confusion Matrix\n","Confusion Matrix provide a nice overlook at the model's performance in classification task"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:43:58.233138Z","iopub.status.busy":"2022-06-11T07:43:58.232746Z","iopub.status.idle":"2022-06-11T07:43:58.462674Z","shell.execute_reply":"2022-06-11T07:43:58.461513Z","shell.execute_reply.started":"2022-06-11T07:43:58.233076Z"},"trusted":true},"outputs":[],"source":["import itertools\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","def plot_confusion_matrix(cm, classes,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.GnBu):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title, fontsize=20)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, fontsize=13)\n","    plt.yticks(tick_marks, classes, fontsize=13)\n","\n","    fmt = '.2f'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label', fontsize=15)\n","    plt.xlabel('Predicted label', fontsize=15)"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:46:58.336546Z","iopub.status.busy":"2022-06-11T07:46:58.335965Z","iopub.status.idle":"2022-06-11T07:46:58.620525Z","shell.execute_reply":"2022-06-11T07:46:58.619405Z","shell.execute_reply.started":"2022-06-11T07:46:58.336308Z"},"trusted":true},"outputs":[],"source":["cnf_matrix1 = confusion_matrix(val_df.sentiment.to_list(), y_pred)\n","plt.figure(figsize=(7,7))\n","plot_confusion_matrix(cnf_matrix1, classes=val_df.sentiment.unique(), title=\"Predict on tweets with suffix\")\n","plt.savefig(\"/kaggle/working/withsuffix.png\")"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:47:09.544150Z","iopub.status.busy":"2022-06-11T07:47:09.543779Z","iopub.status.idle":"2022-06-11T07:47:09.800156Z","shell.execute_reply":"2022-06-11T07:47:09.798634Z","shell.execute_reply.started":"2022-06-11T07:47:09.544095Z"},"trusted":true},"outputs":[],"source":["cnf_matrix2 = confusion_matrix(val_df.sentiment.to_list(), y_pred_only_words)\n","plt.figure(figsize=(7,7))\n","plot_confusion_matrix(cnf_matrix2, classes=val_df.sentiment.unique(), title=\"Predict on tweets without suffix\")\n","plt.savefig(\"/kaggle/working/nosuffix.png\")"]},{"cell_type":"markdown","metadata":{},"source":["### Classification Scores"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T07:26:12.610038Z","iopub.status.busy":"2022-06-11T07:26:12.609571Z","iopub.status.idle":"2022-06-11T07:26:12.637290Z","shell.execute_reply":"2022-06-11T07:26:12.636239Z","shell.execute_reply.started":"2022-06-11T07:26:12.609981Z"},"trusted":true},"outputs":[],"source":["print(classification_report(list(val_df.sentiment), y_pred))\n","print(classification_report(list(val_df.sentiment), y_pred_only_words))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
