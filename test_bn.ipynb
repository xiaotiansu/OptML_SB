{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bb8b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.functional import F\n",
    "from torchvision.transforms import functional\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision\n",
    "from time import time\n",
    "import torch.optim.lr_scheduler as S\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97dbead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385e194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root = 'data/', train = False, download = True)\n",
    "test_input = test_set.data.view(-1, 1, 28, 28).float().to(device)\n",
    "test_targets = test_set.targets.to(device)\n",
    "\n",
    "mu, std = test_input.mean(), test_input.std()\n",
    "test_input.sub_(mu).div_(std)\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root = 'data/', train = True, download = True)\n",
    "train_input = train_set.data.view(-1, 1, 28, 28).float()\n",
    "train_targets = train_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43167c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLeNet(nn.Module):\n",
    "    def __init__(self, use_bn=False):\n",
    "        super(mLeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        last_channel = 16\n",
    "        self.conv3 = nn.Conv2d(16, last_channel, 2)\n",
    "        self.use_bn = use_bn\n",
    "        if self.use_bn:\n",
    "            self.conv1_bn = nn.BatchNorm2d(6)\n",
    "            self.conv2_bn = nn.BatchNorm2d(16)\n",
    "            self.conv3_bn = nn.BatchNorm2d(last_channel)\n",
    "\n",
    "        self.fc3 = nn.Linear(last_channel, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn:\n",
    "            x = F.max_pool2d(F.relu(self.conv1_bn(self.conv1(x))), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv2_bn(self.conv2(x))), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv3_bn(self.conv3(x))), 2)\n",
    "        else:\n",
    "            x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = F.adaptive_avg_pool2d(x,1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9405d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(use_bn = False, lr = 0.005, optimize_method = torch.optim.SGD, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    device = 'cuda'\n",
    "    nb_epochs, batch_size = 3000, 32\n",
    "\n",
    "    model = mLeNet(use_bn = use_bn)\n",
    "    model.to(device)\n",
    "    #optimizer = optimize_method(model.parameters(), lr = lr, momentum = momentum )\n",
    "    optimizer = optimize_method(model.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = S.StepLR(optimizer, step_size=200, gamma=1)\n",
    "\n",
    "    train_len, attack_len = 512, 512\n",
    "    lbd = 0.2\n",
    "\n",
    "    test_acc = []\n",
    "    train_acc = []\n",
    "    l_list = []\n",
    "\n",
    "    criterion.to(device)\n",
    "\n",
    "    train_x, train_y = train_input[:train_len].to(device), train_targets[:train_len].to(device)\n",
    "    mu, std = train_x.mean(), train_x.std()\n",
    "    train_x.sub_(mu).div_(std)\n",
    "\n",
    "    attack_x, attack_y = train_input[train_len:train_len + attack_len].to(device), train_targets[train_len:train_len + attack_len].to(device)\n",
    "    mu, std = attack_x.mean(), attack_x.std()\n",
    "    attack_x.sub_(mu).div_(std)\n",
    "\n",
    "    mylist = list(range(10))\n",
    "    for i in range(attack_len):\n",
    "        n = mylist[:]\n",
    "        n.remove(attack_y[i])\n",
    "        attack_y[i] = random.choice(n)\n",
    "\n",
    "\n",
    "    train_batches = math.ceil(train_len/batch_size)\n",
    "\n",
    "    for e in tqdm(range(nb_epochs)):\n",
    "        ite = 0\n",
    "        for input, targets in zip(train_x.split(batch_size) + attack_x.split(batch_size),train_y.split(batch_size) + attack_y.split(batch_size)):\n",
    "        \n",
    "        #for input, targets in zip(train_x.split(batch_size),train_y.split(batch_size)):\n",
    "            output = model(input)\n",
    "            ite += 1\n",
    "            if ite<=train_batches:\n",
    "                loss = criterion(output, targets)\n",
    "            else:\n",
    "                loss = lbd*criterion(output, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        with torch.no_grad():\n",
    "            out = model(train_x)\n",
    "            _, predicted_classes = out.max(1)\n",
    "            _, predicted_classes_test = model(test_input).max(1)\n",
    "\n",
    "            l = criterion(out, train_y).item()\n",
    "            test_a = (predicted_classes_test == test_targets).to(float).mean().item()\n",
    "            train_a = (predicted_classes == train_y).to(float).mean().item()\n",
    "\n",
    "            l_list.append(l)\n",
    "            test_acc.append(test_a)\n",
    "            train_acc.append(train_a)\n",
    "            \n",
    "            if (e>=500) and (all(i >= 2 for i in l_list[-10:])):\n",
    "                return model,\"diverge\"\n",
    "                break\n",
    "            \n",
    "            print(\"epoch: %i, loss: %.4f, train_acc: %.4f, test_acc: %.4f\" %(e+1,l,train_a,test_a))\n",
    "            stats = [e+1,l,train_a,test_a]\n",
    "            if l <= 0.001:\n",
    "                return model,stats\n",
    "                break\n",
    "    return model,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdef67f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa21ac395c845d0b404d07c2f2a3a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 2.2383, train_acc: 0.2266, test_acc: 0.1765\n",
      "epoch: 2, loss: 2.0429, train_acc: 0.3555, test_acc: 0.2972\n",
      "epoch: 3, loss: 1.6231, train_acc: 0.5449, test_acc: 0.4611\n",
      "epoch: 4, loss: 1.4148, train_acc: 0.5547, test_acc: 0.5049\n",
      "epoch: 5, loss: 1.3883, train_acc: 0.5645, test_acc: 0.4904\n",
      "epoch: 6, loss: 1.6412, train_acc: 0.3652, test_acc: 0.3206\n",
      "epoch: 7, loss: 1.0977, train_acc: 0.7051, test_acc: 0.6187\n",
      "epoch: 8, loss: 1.1603, train_acc: 0.7168, test_acc: 0.6071\n",
      "epoch: 9, loss: 1.0263, train_acc: 0.7852, test_acc: 0.6945\n",
      "epoch: 10, loss: 0.9400, train_acc: 0.8281, test_acc: 0.7274\n",
      "epoch: 11, loss: 0.8250, train_acc: 0.8516, test_acc: 0.7352\n",
      "epoch: 12, loss: 0.8569, train_acc: 0.8359, test_acc: 0.7382\n",
      "epoch: 13, loss: 0.6919, train_acc: 0.8887, test_acc: 0.7764\n",
      "epoch: 14, loss: 0.7843, train_acc: 0.8516, test_acc: 0.7433\n",
      "epoch: 15, loss: 0.7245, train_acc: 0.8691, test_acc: 0.7473\n",
      "epoch: 16, loss: 0.8065, train_acc: 0.8203, test_acc: 0.7097\n",
      "epoch: 17, loss: 0.9642, train_acc: 0.7168, test_acc: 0.6169\n",
      "epoch: 18, loss: 1.0703, train_acc: 0.6641, test_acc: 0.5724\n",
      "epoch: 19, loss: 0.7696, train_acc: 0.8359, test_acc: 0.7320\n",
      "epoch: 20, loss: 0.5311, train_acc: 0.9102, test_acc: 0.7890\n",
      "epoch: 21, loss: 0.4851, train_acc: 0.9219, test_acc: 0.7988\n",
      "epoch: 22, loss: 0.4459, train_acc: 0.9453, test_acc: 0.8196\n",
      "epoch: 23, loss: 0.4310, train_acc: 0.9551, test_acc: 0.8210\n",
      "epoch: 24, loss: 0.3891, train_acc: 0.9570, test_acc: 0.8218\n",
      "epoch: 25, loss: 0.3682, train_acc: 0.9727, test_acc: 0.8271\n",
      "epoch: 26, loss: 0.3487, train_acc: 0.9766, test_acc: 0.8233\n",
      "epoch: 27, loss: 0.3441, train_acc: 0.9707, test_acc: 0.8174\n",
      "epoch: 28, loss: 0.3421, train_acc: 0.9746, test_acc: 0.8050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_83048/422916893.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_bn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmy_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstats_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"experiment_results/stats_data_adamBN.npy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstats_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_83048/3139544359.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(use_bn, lr, optimize_method, seed)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m#for input, targets in zip(train_x.split(batch_size),train_y.split(batch_size)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mite\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mite\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bonan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_83048/878009923.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#x = F.relu(self.fc1(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#x = F.relu(self.fc2(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bonan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bonan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bonan\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimize_method = torch.optim.Adam\n",
    "use_bn = False\n",
    "lr = 0.005\n",
    "stats_data = np.empty([2,20],dtype=object)\n",
    "use_bn_list = [False,True]\n",
    "\n",
    "for ii,use_bn in enumerate(use_bn_list):\n",
    "    for seed in range(20):\n",
    "        print(use_bn, seed)\n",
    "        my_model,stats_data[seed] = train_model(use_bn = use_bn, lr = lr, optimize_method = optimize_method, seed = seed)\n",
    "        np.save(\"experiment_results/stats_data_adamBN.npy\",stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea554f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats_data_adamBN = np.load(\"experiment_results/stats_data_adamBN.npy\", allow_pickle=True)\n",
    "\n",
    "plt.boxplot([[x[-1] for x in stats_data_adamBN[0]],[x[-1] for x in stats_data_adamBN[1]]])\n",
    "plt.xticks([1, 2], ['without BatchNorm', 'with BatchNorm'])\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title('Influence of batch normalization layer')\n",
    "plt.savefig(\"figures/bn.png\",dpi=200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a02900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
