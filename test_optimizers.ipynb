{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a1c8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.functional import F\n",
    "from torchvision.transforms import functional\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision\n",
    "from time import time\n",
    "import torch.optim.lr_scheduler as S\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af0fa2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f7556fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root = 'data/', train = False, download = True)\n",
    "test_input = test_set.data.view(-1, 1, 28, 28).float().to(device)\n",
    "test_targets = test_set.targets.to(device)\n",
    "\n",
    "mu, std = test_input.mean(), test_input.std()\n",
    "test_input.sub_(mu).div_(std)\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root = 'data/', train = True, download = True)\n",
    "train_input = train_set.data.view(-1, 1, 28, 28).float()\n",
    "train_targets = train_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dee1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLeNet(nn.Module):\n",
    "    def __init__(self, use_bn=False):\n",
    "        super(mLeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        last_channel = 16\n",
    "        self.conv3 = nn.Conv2d(16, last_channel, 2)\n",
    "        self.use_bn = use_bn\n",
    "        if self.use_bn:\n",
    "            self.conv1_bn = nn.BatchNorm2d(6)\n",
    "            self.conv2_bn = nn.BatchNorm2d(16)\n",
    "            self.conv3_bn = nn.BatchNorm2d(last_channel)\n",
    "\n",
    "        self.fc3 = nn.Linear(last_channel, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn:\n",
    "            x = F.max_pool2d(F.relu(self.conv1_bn(self.conv1(x))), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv2_bn(self.conv2(x))), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv3_bn(self.conv3(x))), 2)\n",
    "        else:\n",
    "            x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = F.adaptive_avg_pool2d(x,1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db1cf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(use_bn = False, lr = 0.005, optimize_method = torch.optim.Adam, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    device = 'cuda'\n",
    "    nb_epochs, batch_size = 3000, 32\n",
    "\n",
    "    model = mLeNet(use_bn = use_bn)\n",
    "    model.to(device)\n",
    "    optimizer = optimize_method(model.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = S.StepLR(optimizer, step_size=200, gamma=1)\n",
    "\n",
    "    train_len, attack_len = 512, 0\n",
    "    lbd = 0\n",
    "\n",
    "    test_acc = []\n",
    "    train_acc = []\n",
    "    l_list = []\n",
    "\n",
    "    criterion.to(device)\n",
    "\n",
    "    train_x, train_y = train_input[:train_len].to(device), train_targets[:train_len].to(device)\n",
    "    mu, std = train_x.mean(), train_x.std()\n",
    "    train_x.sub_(mu).div_(std)\n",
    "\n",
    "    attack_x, attack_y = train_input[train_len:train_len + attack_len].to(device), train_targets[train_len:train_len + attack_len].to(device)\n",
    "    mu, std = attack_x.mean(), attack_x.std()\n",
    "    attack_x.sub_(mu).div_(std)\n",
    "\n",
    "    mylist = list(range(10))\n",
    "    for i in range(attack_len):\n",
    "        n = mylist[:]\n",
    "        n.remove(attack_y[i])\n",
    "        attack_y[i] = random.choice(n)\n",
    "\n",
    "\n",
    "    train_batches = math.ceil(train_len/batch_size)\n",
    "\n",
    "    for e in tqdm(range(nb_epochs)):\n",
    "        ite = 0\n",
    "        for input, targets in zip(train_x.split(batch_size) + attack_x.split(batch_size),train_y.split(batch_size) + attack_y.split(batch_size)):\n",
    "        \n",
    "        #for input, targets in zip(train_x.split(batch_size),train_y.split(batch_size)):\n",
    "            output = model(input)\n",
    "            ite += 1\n",
    "            if ite<=train_batches:\n",
    "                loss = criterion(output, targets)\n",
    "            else:\n",
    "                loss = lbd*criterion(output, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        with torch.no_grad():\n",
    "            out = model(train_x)\n",
    "            _, predicted_classes = out.max(1)\n",
    "            _, predicted_classes_test = model(test_input).max(1)\n",
    "\n",
    "            l = criterion(out, train_y).item()\n",
    "            test_a = (predicted_classes_test == test_targets).to(float).mean().item()\n",
    "            train_a = (predicted_classes == train_y).to(float).mean().item()\n",
    "\n",
    "            l_list.append(l)\n",
    "            test_acc.append(test_a)\n",
    "            train_acc.append(train_a)\n",
    "            \n",
    "            if (e>=300) and (all(i >= 2 for i in l_list[-10:])):\n",
    "                return model,\"diverge\"\n",
    "                break\n",
    "            \n",
    "            print(\"epoch: %i, loss: %.4f, train_acc: %.4f, test_acc: %.4f\" %(e+1,l,train_a,test_a))\n",
    "            stats = [e+1,l,train_a,test_a]\n",
    "            if l <= 0.001:\n",
    "                return model,stats\n",
    "                break\n",
    "    return model,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "233e68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_fro_approx(model, use_bn = False, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    device = 'cuda'\n",
    "    train_len = 512\n",
    "\n",
    "    train_x, train_y = train_input[:train_len].to(device), train_targets[:train_len].to(device)\n",
    "    mu, std = train_x.mean(), train_x.std()\n",
    "    train_x.sub_(mu).div_(std)\n",
    "    m_up = mLeNet(use_bn = use_bn).to(device)\n",
    "    m_down = mLeNet(use_bn = use_bn).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(device)\n",
    "    eps = 1e-5\n",
    "    M=100\n",
    "    diffs = torch.zeros(M)\n",
    "    for i in range(M):\n",
    "        m_up.zero_grad()\n",
    "        m_down.zero_grad()\n",
    "        for po,p_up,p_down in zip(model.parameters(),m_up.parameters(),m_down.parameters()):\n",
    "            temp = torch.normal(0,1,size=po.shape).to(device)\n",
    "            p_up.data = po + eps*temp\n",
    "            p_down.data = po - eps*temp\n",
    "            #print(p_up.shape,p_down.shape)\n",
    "\n",
    "        loss_up = criterion(m_up(train_x), train_y)\n",
    "        loss_up.backward()\n",
    "\n",
    "        loss_down = criterion(m_down(train_x), train_y)\n",
    "        loss_down.backward()\n",
    "\n",
    "\n",
    "        grad_up = []\n",
    "        grad_down = []\n",
    "\n",
    "        nparams = 0\n",
    "        for p_up,p_down in zip(m_up.parameters(),m_down.parameters()):\n",
    "            grad_up.append(p_up.grad.view(-1).data)\n",
    "            grad_down.append(p_down.grad.view(-1).data)\n",
    "            \n",
    "        diffs[i] = ((torch.concat(grad_up)-torch.concat(grad_down))**2).sum()\n",
    "\n",
    "    return (diffs.sum()/(4*M*eps*eps)).sqrt().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03c1934",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.optim.adagrad.Adagrad'> 0.01 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa90a466bc84f70b2279e6e403d1670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.7476, train_acc: 0.7051, test_acc: 0.5773\n",
      "epoch: 2, loss: 1.5069, train_acc: 0.8203, test_acc: 0.6807\n",
      "epoch: 3, loss: 1.3289, train_acc: 0.8574, test_acc: 0.7233\n",
      "epoch: 4, loss: 1.1834, train_acc: 0.8984, test_acc: 0.7478\n",
      "epoch: 5, loss: 1.0636, train_acc: 0.9102, test_acc: 0.7679\n",
      "epoch: 6, loss: 0.9629, train_acc: 0.9180, test_acc: 0.7855\n",
      "epoch: 7, loss: 0.8817, train_acc: 0.9316, test_acc: 0.7974\n",
      "epoch: 8, loss: 0.8108, train_acc: 0.9492, test_acc: 0.8061\n",
      "epoch: 9, loss: 0.7510, train_acc: 0.9531, test_acc: 0.8151\n",
      "epoch: 10, loss: 0.7000, train_acc: 0.9551, test_acc: 0.8229\n",
      "epoch: 11, loss: 0.6542, train_acc: 0.9590, test_acc: 0.8274\n",
      "epoch: 12, loss: 0.6134, train_acc: 0.9609, test_acc: 0.8329\n",
      "epoch: 13, loss: 0.5764, train_acc: 0.9648, test_acc: 0.8365\n",
      "epoch: 14, loss: 0.5451, train_acc: 0.9648, test_acc: 0.8399\n",
      "epoch: 15, loss: 0.5148, train_acc: 0.9688, test_acc: 0.8422\n",
      "epoch: 16, loss: 0.4883, train_acc: 0.9727, test_acc: 0.8437\n",
      "epoch: 17, loss: 0.4632, train_acc: 0.9746, test_acc: 0.8470\n",
      "epoch: 18, loss: 0.4400, train_acc: 0.9746, test_acc: 0.8495\n",
      "epoch: 19, loss: 0.4196, train_acc: 0.9766, test_acc: 0.8497\n",
      "epoch: 20, loss: 0.4007, train_acc: 0.9805, test_acc: 0.8520\n",
      "epoch: 21, loss: 0.3836, train_acc: 0.9805, test_acc: 0.8541\n",
      "epoch: 22, loss: 0.3664, train_acc: 0.9805, test_acc: 0.8551\n",
      "epoch: 23, loss: 0.3518, train_acc: 0.9824, test_acc: 0.8553\n",
      "epoch: 24, loss: 0.3367, train_acc: 0.9824, test_acc: 0.8569\n",
      "epoch: 25, loss: 0.3243, train_acc: 0.9844, test_acc: 0.8570\n",
      "epoch: 26, loss: 0.3112, train_acc: 0.9844, test_acc: 0.8579\n",
      "epoch: 27, loss: 0.2990, train_acc: 0.9844, test_acc: 0.8588\n",
      "epoch: 28, loss: 0.2876, train_acc: 0.9863, test_acc: 0.8591\n",
      "epoch: 29, loss: 0.2766, train_acc: 0.9863, test_acc: 0.8609\n",
      "epoch: 30, loss: 0.2670, train_acc: 0.9863, test_acc: 0.8617\n",
      "epoch: 31, loss: 0.2572, train_acc: 0.9883, test_acc: 0.8625\n",
      "epoch: 32, loss: 0.2481, train_acc: 0.9902, test_acc: 0.8632\n",
      "epoch: 33, loss: 0.2404, train_acc: 0.9922, test_acc: 0.8632\n",
      "epoch: 34, loss: 0.2318, train_acc: 0.9941, test_acc: 0.8641\n",
      "epoch: 35, loss: 0.2243, train_acc: 0.9961, test_acc: 0.8640\n",
      "epoch: 36, loss: 0.2167, train_acc: 0.9980, test_acc: 0.8652\n",
      "epoch: 37, loss: 0.2102, train_acc: 0.9980, test_acc: 0.8663\n",
      "epoch: 38, loss: 0.2038, train_acc: 0.9980, test_acc: 0.8655\n",
      "epoch: 39, loss: 0.1971, train_acc: 0.9980, test_acc: 0.8671\n",
      "epoch: 40, loss: 0.1915, train_acc: 0.9980, test_acc: 0.8662\n",
      "epoch: 41, loss: 0.1857, train_acc: 0.9980, test_acc: 0.8668\n",
      "epoch: 42, loss: 0.1805, train_acc: 0.9980, test_acc: 0.8670\n",
      "epoch: 43, loss: 0.1753, train_acc: 0.9980, test_acc: 0.8676\n",
      "epoch: 44, loss: 0.1704, train_acc: 0.9980, test_acc: 0.8677\n",
      "epoch: 45, loss: 0.1658, train_acc: 0.9980, test_acc: 0.8684\n",
      "epoch: 46, loss: 0.1613, train_acc: 1.0000, test_acc: 0.8676\n",
      "epoch: 47, loss: 0.1571, train_acc: 1.0000, test_acc: 0.8673\n",
      "epoch: 48, loss: 0.1530, train_acc: 1.0000, test_acc: 0.8673\n",
      "epoch: 49, loss: 0.1491, train_acc: 1.0000, test_acc: 0.8675\n",
      "epoch: 50, loss: 0.1455, train_acc: 1.0000, test_acc: 0.8673\n",
      "epoch: 51, loss: 0.1419, train_acc: 1.0000, test_acc: 0.8671\n",
      "epoch: 52, loss: 0.1387, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 53, loss: 0.1353, train_acc: 1.0000, test_acc: 0.8678\n",
      "epoch: 54, loss: 0.1320, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 55, loss: 0.1291, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 56, loss: 0.1262, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 57, loss: 0.1234, train_acc: 1.0000, test_acc: 0.8681\n",
      "epoch: 58, loss: 0.1205, train_acc: 1.0000, test_acc: 0.8678\n",
      "epoch: 59, loss: 0.1179, train_acc: 1.0000, test_acc: 0.8682\n",
      "epoch: 60, loss: 0.1154, train_acc: 1.0000, test_acc: 0.8678\n",
      "epoch: 61, loss: 0.1129, train_acc: 1.0000, test_acc: 0.8683\n",
      "epoch: 62, loss: 0.1106, train_acc: 1.0000, test_acc: 0.8675\n",
      "epoch: 63, loss: 0.1082, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 64, loss: 0.1060, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 65, loss: 0.1039, train_acc: 1.0000, test_acc: 0.8681\n",
      "epoch: 66, loss: 0.1019, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 67, loss: 0.0999, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 68, loss: 0.0979, train_acc: 1.0000, test_acc: 0.8682\n",
      "epoch: 69, loss: 0.0961, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 70, loss: 0.0942, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 71, loss: 0.0925, train_acc: 1.0000, test_acc: 0.8681\n",
      "epoch: 72, loss: 0.0909, train_acc: 1.0000, test_acc: 0.8683\n",
      "epoch: 73, loss: 0.0891, train_acc: 1.0000, test_acc: 0.8678\n",
      "epoch: 74, loss: 0.0876, train_acc: 1.0000, test_acc: 0.8683\n",
      "epoch: 75, loss: 0.0861, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 76, loss: 0.0845, train_acc: 1.0000, test_acc: 0.8682\n",
      "epoch: 77, loss: 0.0831, train_acc: 1.0000, test_acc: 0.8683\n",
      "epoch: 78, loss: 0.0817, train_acc: 1.0000, test_acc: 0.8685\n",
      "epoch: 79, loss: 0.0803, train_acc: 1.0000, test_acc: 0.8683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_82536/1641648120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimize_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mmy_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstats_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mHessians\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH_fro_approx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHessians\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_82536/3155588777.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(use_bn, lr, optimize_method, seed)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bonan\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bonan\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_seed = 10\n",
    "stats_data = np.empty([3,num_seed],dtype=object)\n",
    "Hessians = np.empty([3,num_seed],dtype=object)\n",
    "optimizer_list = [torch.optim.Adagrad,torch.optim.SGD,torch.optim.Adam]\n",
    "lr = 0.01\n",
    "use_bn = True\n",
    "seed_list = list(range(num_seed))\n",
    "for ii, optimize_method in enumerate(optimizer_list):\n",
    "    for ll, seed in enumerate(seed_list):\n",
    "        print(optimize_method,lr,seed)\n",
    "        my_model,stats_data[ii,ll] = train_model(use_bn = use_bn, lr = lr, optimize_method = optimize_method, seed = seed)\n",
    "        Hessians[ii,ll] = H_fro_approx(my_model,use_bn = use_bn, seed = seed)\n",
    "        print(Hessians[ii,ll])\n",
    "        np.save(\"experiment_results/hessians_op.npy\",Hessians)\n",
    "        np.save(\"experiment_results/stats_data_op.npy\",stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "689beb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hessians = np.load(\"experiment_results/hessians_op.npy\", allow_pickle = True)\n",
    "stats_data = np.load(\"experiment_results/stats_data_op.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efd660a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Hessians[2], [x[-1] for x in stats_data[2]], label = \"Adam\", alpha=0.5)\n",
    "plt.scatter(Hessians[0], [x[-1] for x in stats_data[0]], label = \"Adagrad\", alpha=0.5)\n",
    "plt.scatter(Hessians[1], [x[-1] for x in stats_data[1]], label = \"SGD\", alpha=0.5)\n",
    "\n",
    "plt.gca().set_prop_cycle(None)\n",
    "\n",
    "plt.scatter(Hessians[2].mean(), np.mean([x[-1] for x in stats_data[2]]), marker = \"X\", s=100)\n",
    "plt.scatter(Hessians[0].mean(), np.mean([x[-1] for x in stats_data[0]]), marker = \"X\", s=100)\n",
    "plt.scatter(Hessians[1].mean(), np.mean([x[-1] for x in stats_data[1]]), marker = \"X\", s=100)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/optimizer.png\",dpi=200,bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
