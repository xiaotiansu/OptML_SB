{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1c8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.functional import F\n",
    "from torchvision.transforms import functional\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision\n",
    "from time import time\n",
    "import torch.optim.lr_scheduler as S\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af0fa2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7556fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root = 'data/', train = False, download = True)\n",
    "test_input = test_set.data.view(-1, 1, 28, 28).float().to(device)\n",
    "test_targets = test_set.targets.to(device)\n",
    "\n",
    "mu, std = test_input.mean(), test_input.std()\n",
    "test_input.sub_(mu).div_(std)\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root = 'data/', train = True, download = True)\n",
    "train_input = train_set.data.view(-1, 1, 28, 28).float()\n",
    "train_targets = train_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dee1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLeNet(nn.Module):\n",
    "    def __init__(self, use_bn=False):\n",
    "        super(mLeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        last_channel = 16\n",
    "        self.conv3 = nn.Conv2d(16, last_channel, 2)\n",
    "        self.use_bn = use_bn\n",
    "        if self.use_bn:\n",
    "            self.conv1_bn = nn.BatchNorm2d(6)\n",
    "            self.conv2_bn = nn.BatchNorm2d(16)\n",
    "            self.conv3_bn = nn.BatchNorm2d(last_channel)\n",
    "\n",
    "        self.fc3 = nn.Linear(last_channel, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn:\n",
    "            x = F.max_pool2d(F.relu(self.conv1_bn(self.conv1(x))), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv2_bn(self.conv2(x))), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv3_bn(self.conv3(x))), 2)\n",
    "        else:\n",
    "            x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "            x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = F.adaptive_avg_pool2d(x,1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db1cf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(use_bn = False, lr = 0.005, optimize_method = torch.optim.Adam, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    device = 'cuda'\n",
    "    nb_epochs, batch_size = 3000, 32\n",
    "\n",
    "    model = mLeNet(use_bn = use_bn)\n",
    "    model.to(device)\n",
    "    optimizer = optimize_method(model.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = S.StepLR(optimizer, step_size=200, gamma=1)\n",
    "\n",
    "    train_len, attack_len = 512, 0\n",
    "    lbd = 0\n",
    "\n",
    "    test_acc = []\n",
    "    train_acc = []\n",
    "    l_list = []\n",
    "\n",
    "    criterion.to(device)\n",
    "\n",
    "    train_x, train_y = train_input[:train_len].to(device), train_targets[:train_len].to(device)\n",
    "    mu, std = train_x.mean(), train_x.std()\n",
    "    train_x.sub_(mu).div_(std)\n",
    "\n",
    "    attack_x, attack_y = train_input[train_len:train_len + attack_len].to(device), train_targets[train_len:train_len + attack_len].to(device)\n",
    "    mu, std = attack_x.mean(), attack_x.std()\n",
    "    attack_x.sub_(mu).div_(std)\n",
    "\n",
    "    mylist = list(range(10))\n",
    "    for i in range(attack_len):\n",
    "        n = mylist[:]\n",
    "        n.remove(attack_y[i])\n",
    "        attack_y[i] = random.choice(n)\n",
    "\n",
    "\n",
    "    train_batches = math.ceil(train_len/batch_size)\n",
    "\n",
    "    for e in tqdm(range(nb_epochs)):\n",
    "        ite = 0\n",
    "        for input, targets in zip(train_x.split(batch_size) + attack_x.split(batch_size),train_y.split(batch_size) + attack_y.split(batch_size)):\n",
    "        \n",
    "        #for input, targets in zip(train_x.split(batch_size),train_y.split(batch_size)):\n",
    "            output = model(input)\n",
    "            ite += 1\n",
    "            if ite<=train_batches:\n",
    "                loss = criterion(output, targets)\n",
    "            else:\n",
    "                loss = lbd*criterion(output, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        with torch.no_grad():\n",
    "            out = model(train_x)\n",
    "            _, predicted_classes = out.max(1)\n",
    "            _, predicted_classes_test = model(test_input).max(1)\n",
    "\n",
    "            l = criterion(out, train_y).item()\n",
    "            test_a = (predicted_classes_test == test_targets).to(float).mean().item()\n",
    "            train_a = (predicted_classes == train_y).to(float).mean().item()\n",
    "\n",
    "            l_list.append(l)\n",
    "            test_acc.append(test_a)\n",
    "            train_acc.append(train_a)\n",
    "            \n",
    "            if (e>=300) and (all(i >= 2 for i in l_list[-10:])):\n",
    "                return model,\"diverge\"\n",
    "                break\n",
    "            \n",
    "            print(\"epoch: %i, loss: %.4f, train_acc: %.4f, test_acc: %.4f\" %(e+1,l,train_a,test_a))\n",
    "            stats = [e+1,l,train_a,test_a]\n",
    "            if l <= 0.001:\n",
    "                return model,stats\n",
    "                break\n",
    "    return model,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "233e68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_fro_approx(model, use_bn = False, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    device = 'cuda'\n",
    "    train_len = 512\n",
    "\n",
    "    train_x, train_y = train_input[:train_len].to(device), train_targets[:train_len].to(device)\n",
    "    mu, std = train_x.mean(), train_x.std()\n",
    "    train_x.sub_(mu).div_(std)\n",
    "    m_up = mLeNet(use_bn = use_bn).to(device)\n",
    "    m_down = mLeNet(use_bn = use_bn).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(device)\n",
    "    eps = 1e-5\n",
    "    M=100\n",
    "    diffs = torch.zeros(M)\n",
    "    for i in range(M):\n",
    "        m_up.zero_grad()\n",
    "        m_down.zero_grad()\n",
    "        for po,p_up,p_down in zip(model.parameters(),m_up.parameters(),m_down.parameters()):\n",
    "            temp = torch.normal(0,1,size=po.shape).to(device)\n",
    "            p_up.data = po + eps*temp\n",
    "            p_down.data = po - eps*temp\n",
    "            #print(p_up.shape,p_down.shape)\n",
    "\n",
    "        loss_up = criterion(m_up(train_x), train_y)\n",
    "        loss_up.backward()\n",
    "\n",
    "        loss_down = criterion(m_down(train_x), train_y)\n",
    "        loss_down.backward()\n",
    "\n",
    "\n",
    "        grad_up = []\n",
    "        grad_down = []\n",
    "\n",
    "        nparams = 0\n",
    "        for p_up,p_down in zip(m_up.parameters(),m_down.parameters()):\n",
    "            grad_up.append(p_up.grad.view(-1).data)\n",
    "            grad_down.append(p_down.grad.view(-1).data)\n",
    "            \n",
    "        diffs[i] = ((torch.concat(grad_up)-torch.concat(grad_down))**2).sum()\n",
    "\n",
    "    return (diffs.sum()/(4*M*eps*eps)).sqrt().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03c1934",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.optim.adagrad.Adagrad'> 0.01 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa90a466bc84f70b2279e6e403d1670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.7476, train_acc: 0.7051, test_acc: 0.5773\n",
      "epoch: 2, loss: 1.5069, train_acc: 0.8203, test_acc: 0.6807\n",
      "epoch: 3, loss: 1.3289, train_acc: 0.8574, test_acc: 0.7233\n",
      "epoch: 4, loss: 1.1834, train_acc: 0.8984, test_acc: 0.7478\n",
      "epoch: 5, loss: 1.0636, train_acc: 0.9102, test_acc: 0.7679\n",
      "epoch: 6, loss: 0.9629, train_acc: 0.9180, test_acc: 0.7855\n",
      "epoch: 7, loss: 0.8817, train_acc: 0.9316, test_acc: 0.7974\n",
      "epoch: 8, loss: 0.8108, train_acc: 0.9492, test_acc: 0.8061\n",
      "epoch: 9, loss: 0.7510, train_acc: 0.9531, test_acc: 0.8151\n",
      "epoch: 10, loss: 0.7000, train_acc: 0.9551, test_acc: 0.8229\n",
      "epoch: 11, loss: 0.6542, train_acc: 0.9590, test_acc: 0.8274\n",
      "epoch: 12, loss: 0.6134, train_acc: 0.9609, test_acc: 0.8329\n",
      "epoch: 13, loss: 0.5764, train_acc: 0.9648, test_acc: 0.8365\n",
      "epoch: 14, loss: 0.5451, train_acc: 0.9648, test_acc: 0.8399\n",
      "epoch: 15, loss: 0.5148, train_acc: 0.9688, test_acc: 0.8422\n",
      "epoch: 16, loss: 0.4883, train_acc: 0.9727, test_acc: 0.8437\n",
      "epoch: 17, loss: 0.4632, train_acc: 0.9746, test_acc: 0.8470\n",
      "epoch: 18, loss: 0.4400, train_acc: 0.9746, test_acc: 0.8495\n",
      "epoch: 19, loss: 0.4196, train_acc: 0.9766, test_acc: 0.8497\n",
      "epoch: 20, loss: 0.4007, train_acc: 0.9805, test_acc: 0.8520\n",
      "epoch: 21, loss: 0.3836, train_acc: 0.9805, test_acc: 0.8541\n",
      "epoch: 22, loss: 0.3664, train_acc: 0.9805, test_acc: 0.8551\n",
      "epoch: 23, loss: 0.3518, train_acc: 0.9824, test_acc: 0.8553\n",
      "epoch: 24, loss: 0.3367, train_acc: 0.9824, test_acc: 0.8569\n",
      "epoch: 25, loss: 0.3243, train_acc: 0.9844, test_acc: 0.8570\n",
      "epoch: 26, loss: 0.3112, train_acc: 0.9844, test_acc: 0.8579\n",
      "epoch: 27, loss: 0.2990, train_acc: 0.9844, test_acc: 0.8588\n",
      "epoch: 28, loss: 0.2876, train_acc: 0.9863, test_acc: 0.8591\n",
      "epoch: 29, loss: 0.2766, train_acc: 0.9863, test_acc: 0.8609\n",
      "epoch: 30, loss: 0.2670, train_acc: 0.9863, test_acc: 0.8617\n",
      "epoch: 31, loss: 0.2572, train_acc: 0.9883, test_acc: 0.8625\n",
      "epoch: 32, loss: 0.2481, train_acc: 0.9902, test_acc: 0.8632\n",
      "epoch: 33, loss: 0.2404, train_acc: 0.9922, test_acc: 0.8632\n",
      "epoch: 34, loss: 0.2318, train_acc: 0.9941, test_acc: 0.8641\n",
      "epoch: 35, loss: 0.2243, train_acc: 0.9961, test_acc: 0.8640\n",
      "epoch: 36, loss: 0.2167, train_acc: 0.9980, test_acc: 0.8652\n",
      "epoch: 37, loss: 0.2102, train_acc: 0.9980, test_acc: 0.8663\n",
      "epoch: 38, loss: 0.2038, train_acc: 0.9980, test_acc: 0.8655\n",
      "epoch: 39, loss: 0.1971, train_acc: 0.9980, test_acc: 0.8671\n",
      "epoch: 40, loss: 0.1915, train_acc: 0.9980, test_acc: 0.8662\n",
      "epoch: 41, loss: 0.1857, train_acc: 0.9980, test_acc: 0.8668\n",
      "epoch: 42, loss: 0.1805, train_acc: 0.9980, test_acc: 0.8670\n",
      "epoch: 43, loss: 0.1753, train_acc: 0.9980, test_acc: 0.8676\n",
      "epoch: 44, loss: 0.1704, train_acc: 0.9980, test_acc: 0.8677\n",
      "epoch: 45, loss: 0.1658, train_acc: 0.9980, test_acc: 0.8684\n",
      "epoch: 46, loss: 0.1613, train_acc: 1.0000, test_acc: 0.8676\n",
      "epoch: 47, loss: 0.1571, train_acc: 1.0000, test_acc: 0.8673\n",
      "epoch: 48, loss: 0.1530, train_acc: 1.0000, test_acc: 0.8673\n",
      "epoch: 49, loss: 0.1491, train_acc: 1.0000, test_acc: 0.8675\n",
      "epoch: 50, loss: 0.1455, train_acc: 1.0000, test_acc: 0.8673\n",
      "epoch: 51, loss: 0.1419, train_acc: 1.0000, test_acc: 0.8671\n",
      "epoch: 52, loss: 0.1387, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 53, loss: 0.1353, train_acc: 1.0000, test_acc: 0.8678\n",
      "epoch: 54, loss: 0.1320, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 55, loss: 0.1291, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 56, loss: 0.1262, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 57, loss: 0.1234, train_acc: 1.0000, test_acc: 0.8681\n",
      "epoch: 58, loss: 0.1205, train_acc: 1.0000, test_acc: 0.8678\n",
      "epoch: 59, loss: 0.1179, train_acc: 1.0000, test_acc: 0.8682\n",
      "epoch: 60, loss: 0.1154, train_acc: 1.0000, test_acc: 0.8678\n",
      "epoch: 61, loss: 0.1129, train_acc: 1.0000, test_acc: 0.8683\n",
      "epoch: 62, loss: 0.1106, train_acc: 1.0000, test_acc: 0.8675\n",
      "epoch: 63, loss: 0.1082, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 64, loss: 0.1060, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 65, loss: 0.1039, train_acc: 1.0000, test_acc: 0.8681\n",
      "epoch: 66, loss: 0.1019, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 67, loss: 0.0999, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 68, loss: 0.0979, train_acc: 1.0000, test_acc: 0.8682\n",
      "epoch: 69, loss: 0.0961, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 70, loss: 0.0942, train_acc: 1.0000, test_acc: 0.8680\n",
      "epoch: 71, loss: 0.0925, train_acc: 1.0000, test_acc: 0.8681\n",
      "epoch: 72, loss: 0.0909, train_acc: 1.0000, test_acc: 0.8683\n",
      "epoch: 73, loss: 0.0891, train_acc: 1.0000, test_acc: 0.8678\n",
      "epoch: 74, loss: 0.0876, train_acc: 1.0000, test_acc: 0.8683\n",
      "epoch: 75, loss: 0.0861, train_acc: 1.0000, test_acc: 0.8679\n",
      "epoch: 76, loss: 0.0845, train_acc: 1.0000, test_acc: 0.8682\n",
      "epoch: 77, loss: 0.0831, train_acc: 1.0000, test_acc: 0.8683\n",
      "epoch: 78, loss: 0.0817, train_acc: 1.0000, test_acc: 0.8685\n",
      "epoch: 79, loss: 0.0803, train_acc: 1.0000, test_acc: 0.8683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_82536/1641648120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimize_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mmy_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstats_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mHessians\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH_fro_approx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHessians\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_82536/3155588777.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(use_bn, lr, optimize_method, seed)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bonan\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bonan\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_seed = 10\n",
    "stats_data = np.empty([3,num_seed],dtype=object)\n",
    "Hessians = np.empty([3,num_seed],dtype=object)\n",
    "optimizer_list = [torch.optim.Adagrad,torch.optim.SGD,torch.optim.Adam]\n",
    "lr = 0.01\n",
    "use_bn = True\n",
    "seed_list = list(range(num_seed))\n",
    "for ii, optimize_method in enumerate(optimizer_list):\n",
    "    for ll, seed in enumerate(seed_list):\n",
    "        print(optimize_method,lr,seed)\n",
    "        my_model,stats_data[ii,ll] = train_model(use_bn = use_bn, lr = lr, optimize_method = optimize_method, seed = seed)\n",
    "        Hessians[ii,ll] = H_fro_approx(my_model,use_bn = use_bn, seed = seed)\n",
    "        print(Hessians[ii,ll])\n",
    "        np.save(\"experiment_results/hessians_op.npy\",Hessians)\n",
    "        np.save(\"experiment_results/stats_data_op.npy\",stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd660a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hessians = np.load(\"experiment_results/hessians_op.npy\", allow_pickle = True)\n",
    "stats_data = np.load(\"experiment_results/stats_data_op.npy\", allow_pickle = True)\n",
    "\n",
    "plt.scatter(Hessians[2], [x[-1] for x in stats_data[2]], label = \"Adam\", alpha=0.5)\n",
    "plt.scatter(Hessians[0], [x[-1] for x in stats_data[0]], label = \"Adagrad\", alpha=0.5)\n",
    "plt.scatter(Hessians[1], [x[-1] for x in stats_data[1]], label = \"SGD\", alpha=0.5)\n",
    "\n",
    "plt.gca().set_prop_cycle(None)\n",
    "\n",
    "plt.scatter(Hessians[2].mean(), np.mean([x[-1] for x in stats_data[2]]), marker = \"X\", s=100)\n",
    "plt.scatter(Hessians[0].mean(), np.mean([x[-1] for x in stats_data[0]]), marker = \"X\", s=100)\n",
    "plt.scatter(Hessians[1].mean(), np.mean([x[-1] for x in stats_data[1]]), marker = \"X\", s=100)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Hessian norm')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('Influence of different optimizers')\n",
    "plt.savefig(\"figures/optimizer.png\",dpi=200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cb630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
